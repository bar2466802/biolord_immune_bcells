{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "60c70a88",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Train a `bioLORD` model with `developing human immune across tissue` for `bioLORD` (B-cells)\n",
    "\n",
    "The data was generated by Suo et al.[[1]](https://www.science.org/doi/full/10.1126/science.abo0510) and downloaded from [Lymphoid cells](https://cellgeni.cog.sanger.ac.uk/developmentcellatlas/fetal-immune/PAN.A01.v01.raw_count.20210429.LYMPHOID.embedding.h5ad). <br>\n",
    "The complete dataset contains a cross-tissue single-cell atlas of developing human immune cells across prenatal hematopoietic, lymphoid, and nonlymphoid peripheral organs. This includes over 900,000 cells from which we identified over 100 cell states.\n",
    "\n",
    "[[1] Suo, Chenqu, Emma Dann, Issac Goh, Laura Jardine, Vitalii Kleshchevnikov, Jong-Eun Park, Rachel A. Botting et al. \"Mapping the developing human immune system across organs.\" Science (2022): eabo0510.](https://www.science.org/doi/full/10.1126/science.abo0510)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "therapeutic-environment",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "crucial-biology",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "sys.path.append(\"/cs/usr/bar246802/bar246802/SandBox2023/biolord_immune_bcells/utils\") # add utils\n",
    "sys.path.append(\"/cs/usr/bar246802/bar246802/SandBox2023/biolord\") # set path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "organized-judges",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[rank: 0] Global seed set to 0\n"
     ]
    }
   ],
   "source": [
    "import biolord\n",
    "import scanpy as sc\n",
    "import anndata\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from os.path import exists\n",
    "import torch\n",
    "import umap.plot\n",
    "import seaborn as sns\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "from cluster_analysis import *\n",
    "from formatters import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "published-royalty",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 1.11.0\n",
      "Using device: gpu\n"
     ]
    }
   ],
   "source": [
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "# Set the device      \n",
    "device = \"gpu\" if torch.backends.cuda.is_built() else \"cpu\"\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "accepted-spell",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tqdm.std.tqdm at 0x7f10fd519790>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from tqdm import tqdm\n",
    "tqdm(disable=True, total=0)  # initialise internal lock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "planned-ivory",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "import mplscience\n",
    "mplscience.set_style()\n",
    "\n",
    "plt.rcParams['legend.scatterpoints'] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "resistant-journalist",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Set parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "driving-craft",
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "DATA_DIR = \"../data/\"\n",
    "SAVE_DIR = \"../output/\"\n",
    "FIG_DIR = \"../figures/\"\n",
    "LOGS_CSV = SAVE_DIR + \"trained_models_scores.csv\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "united-flood",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Import processed data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adata = sc.read(DATA_DIR + \"2_biolord_immune_bcells_bm.h5ad\")"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "adata.obs[\"split\"].value_counts()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cluster_evaluate_figures(attribute_):\n",
    "    ground_truth_labels = np.array(df[attribute_ + '_key'])\n",
    "    print(\"Number of samples:\", ground_truth_labels.size)\n",
    "    title = \"Attribute: \" + attribute_ \n",
    "    path = FIG_DIR + attribute_ + \"_\"\n",
    "    scores = matrices_figures(transf_embeddings_attributes, ground_truth_labels, df,\n",
    "                        attributes_map_rev, attribute_, title, path)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def cluster_evaluate(model, id_, attributes = ['celltype', 'organ']):\n",
    "    transf_embeddings_attributes, df = get_transf_embeddings_attributes(model)\n",
    "    attributes_ground_truth_labels = {'attributes': [], 'true_labels': []}\n",
    "    for attribute in attributes:\n",
    "        ground_truth_labels = np.array(df[attribute + '_key'])\n",
    "        attributes_ground_truth_labels['attributes'].append(attribute)\n",
    "        attributes_ground_truth_labels['true_labels'].append(ground_truth_labels)\n",
    "        ground_truth_unique_labels = list(set(ground_truth_labels))\n",
    "        print(f'For attribute {attribute} the # of unique true labels is: {len(ground_truth_unique_labels)}')\n",
    "\n",
    "    path = SAVE_DIR + \"kmeans_models_scores.csv\"\n",
    "    n_clusters_range = np.arange(2, 16).astype(int)\n",
    "    scores = get_kmeans_score(transf_embeddings_attributes, attributes_ground_truth_labels, n_clusters_range=n_clusters_range, id_=id_, save_path=path)\n",
    "    cols = ['score_name', 'score', 'n_clusters']\n",
    "    all_scores = scores[cols]\n",
    "    print(all_scores)\n",
    "    return all_scores"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def split_adata_into_train_test():\n",
    "    from sklearn.model_selection import train_test_split\n",
    "    adata.obs['split'] = 'nan'\n",
    "    ood_samples = adata.obs.sample(frac = 0.0025, random_state=42).index\n",
    "    adata.obs.loc[ood_samples, \"split\"] = 'ood'\n",
    "\n",
    "    adata_idx = adata.obs_names[adata.obs[\"split\"] != 'ood']\n",
    "    adata_idx_train, adata_idx_test = train_test_split(adata_idx, test_size=0.1, random_state=42)\n",
    "    adata.obs.loc[adata_idx_train, \"split\"] = 'train'\n",
    "    adata.obs.loc[adata_idx_test, \"split\"] = 'test'\n",
    "    a = adata.obs['split'].value_counts()\n",
    "    print(\"Simaple value count of train, test, OOD:\")\n",
    "    print(a)\n",
    "    print(\"\\n\")\n",
    "    print(\"Train, test, OOD by percentage:\")\n",
    "    p = adata.obs['split'].value_counts(normalize=True) * 100\n",
    "    print(p)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def train_model(module_params, trainer_params):\n",
    "    # before each train we wish to re-split the data to make sure we are not biased to a certain split\n",
    "    split_adata_into_train_test()\n",
    "    model = biolord.Biolord(\n",
    "        adata=adata,\n",
    "        n_latent=32,\n",
    "        model_name=\"immune_bcells\",\n",
    "        module_params=module_params,\n",
    "        train_classifiers=False,\n",
    "        split_key=\"split\",\n",
    "    )\n",
    "\n",
    "    model.train(max_epochs=1000,\n",
    "            use_gpu=True,\n",
    "            batch_size=512,\n",
    "            plan_kwargs=trainer_params,\n",
    "            early_stopping=True,\n",
    "            early_stopping_patience=20,\n",
    "            check_val_every_n_epoch=10,\n",
    "            enable_checkpointing=False,\n",
    "            num_workers=1)\n",
    "    return model"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_model_id():\n",
    "    id_ = 1\n",
    "    if exists(LOGS_CSV):\n",
    "        df_logs = pd.read_csv(LOGS_CSV)\n",
    "        id_ = df_logs['id_'].max()\n",
    "        if str(id_).isnumeric():\n",
    "            id_ += 1\n",
    "        else:\n",
    "            id_ = 1\n",
    "    return id_"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "arr_n_latent_attribute_categorical = 2 ** np.arange(4, 8)\n",
    "arr_reconstruction_penalty = [1e-1, 1e1, 1e2, 1e3]\n",
    "arr_unknown_attribute_penalty = [1e-1, 1e1, 1e2, 1e3]\n",
    "arr_unknown_attribute_noise_param = [1e-1, 1e1, 1e2, 1e3]\n",
    "\n",
    "parms_combos = itertools.product(arr_n_latent_attribute_categorical,\n",
    "                                 arr_reconstruction_penalty,\n",
    "                                 arr_unknown_attribute_penalty,\n",
    "                                 arr_unknown_attribute_noise_param)\n",
    "for i, n in enumerate(parms_combos):\n",
    "    if i < 10:\n",
    "        continue\n",
    "    print(n, 'i=', i)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def model_training_iterations():\n",
    "    # arr_n_latent_attribute_categorical = np.concatenate(\n",
    "    #     (np.arange(3, 5, 1), np.arange(5, 31, 5)))\n",
    "    # arr_reconstruction_penalty = [1e1, 1e2, 1e3]\n",
    "    # arr_unknown_attribute_penalty = [1e-2, 1e-1, 1e1]\n",
    "    # arr_unknown_attribute_noise_param = [1e-2, 1e-1, 1e1]\n",
    "\n",
    "    arr_n_latent_attribute_categorical = 2 ** np.arange(4, 8)\n",
    "    arr_reconstruction_penalty = [1e-1, 1e1, 1e2, 1e3]\n",
    "    arr_unknown_attribute_penalty = [1e-1, 1e1, 1e2, 1e3]\n",
    "    arr_unknown_attribute_noise_param = [1e-1, 1e1, 1e2, 1e3]\n",
    "    models_of_interests = [51, 199, 254]\n",
    "    \n",
    "    parms_combos = itertools.product(arr_n_latent_attribute_categorical,\n",
    "                                     arr_reconstruction_penalty,\n",
    "                                     arr_unknown_attribute_penalty,\n",
    "                                     arr_unknown_attribute_noise_param)\n",
    "    id_ = get_model_id()\n",
    "    for i, (n_latent_attribute_categorical, reconstruction_penalty,\n",
    "            unknown_attribute_penalty, unknown_attribute_noise_param\n",
    "             ) in enumerate(parms_combos):\n",
    "        if (i + 1 < id_) or i not in models_of_interests:\n",
    "            continue\n",
    "        print(\n",
    "            f'n_latent_attribute_categorical = {n_latent_attribute_categorical}, reconstruction_penalty = {reconstruction_penalty},unknown_attribute_penalty = {unknown_attribute_penalty}, unknown_attribute_noise_param = {unknown_attribute_noise_param}, i={i+1}'\n",
    "        )\n",
    "\n",
    "        biolord.Biolord.setup_anndata(\n",
    "            adata,\n",
    "            categorical_attributes_keys=[\"celltype\", \"organ\", \"age\"],\n",
    "            retrieval_attribute_key=\"sex\",\n",
    "        )\n",
    "\n",
    "        module_params = {\n",
    "            \"autoencoder_width\": 128,\n",
    "            \"autoencoder_depth\": 2,\n",
    "            \"attribute_nn_width\": 256,\n",
    "            \"attribute_nn_depth\": 2,\n",
    "            \"n_latent_attribute_categorical\": n_latent_attribute_categorical,\n",
    "            \"loss_ae\": \"gauss\",\n",
    "            \"loss_ordered_attribute\": \"gauss\",\n",
    "            \"reconstruction_penalty\": reconstruction_penalty,\n",
    "            \"unknown_attribute_penalty\": unknown_attribute_penalty,\n",
    "            \"unknown_attribute_noise_param\": unknown_attribute_noise_param,\n",
    "            \"attribute_dropout_rate\": 0.1,\n",
    "            \"use_batch_norm\": False,\n",
    "            \"use_layer_norm\": False,\n",
    "            \"seed\": 42,\n",
    "        }\n",
    "\n",
    "        trainer_params = {\n",
    "            \"n_epochs_warmup\": 0,\n",
    "            \"autoencoder_lr\": 1e-4,\n",
    "            \"autoencoder_wd\": 1e-4,\n",
    "            \"attribute_nn_lr\": 1e-2,\n",
    "            \"attribute_nn_wd\": 4e-8,\n",
    "            \"step_size_lr\": 45,\n",
    "            \"cosine_scheduler\": True,\n",
    "            \"scheduler_final_lr\": 1e-5,\n",
    "        }\n",
    "        model = train_model(module_params, trainer_params)\n",
    "        scores = cluster_evaluate(model, id_)\n",
    "        scores[\n",
    "            'n_latent_attribute_categorical'] = n_latent_attribute_categorical\n",
    "        scores['reconstruction_penalty'] = reconstruction_penalty\n",
    "        scores['unknown_attribute_penalty'] = unknown_attribute_penalty\n",
    "        scores['unknown_attribute_noise_param'] = unknown_attribute_noise_param\n",
    "        scores['id_'] = id_\n",
    "        scores = pd.DataFrame(scores)\n",
    "        model.save(SAVE_DIR + \"trained_model_\" + str(id_), overwrite=True)\n",
    "        if id_ == 1 or not exists(LOGS_CSV):\n",
    "            scores.to_csv(LOGS_CSV)\n",
    "        else:\n",
    "            scores.to_csv(LOGS_CSV, mode='a', header=False)\n",
    "        id_ += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "appropriate-option",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_training_iterations()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "democratic-details",
   "metadata": {},
   "source": [
    "## Export genes from raw adata"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "verbal-lending",
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_file_name = \"biolord_immune_bcells_bm_raw\"\n",
    "adata_raw = sc.read(DATA_DIR + \"biolord_immune_bcells_bm.h5ad\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "golden-court",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(adata_raw)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "greenhouse-fence",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genes_raw = set(adata_raw.var['GeneName'])\n",
    "print(f\"numer of unique genes in raw adata file is: {len(unique_genes_raw)}\")\n",
    "df_unique_genes_raw = pd.DataFrame(unique_genes_raw, columns=['GeneName']).sort_values(by=['GeneName'], ascending=True)\n",
    "print(df_unique_genes_raw)\n",
    "df_unique_genes_raw.to_csv(SAVE_DIR + \"unique_genes_raw_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-diagnosis",
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_genes = set(adata.var['GeneName'])\n",
    "print(f\"numer of unique genes in filtered adata file is: {len(unique_genes)}\")\n",
    "df_unique_genes = pd.DataFrame(unique_genes, columns=['GeneName']).sort_values(by=['GeneName'], ascending=True)\n",
    "print(df_unique_genes)\n",
    "df_unique_genes.to_csv(SAVE_DIR + \"unique_genes_flt_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "democratic-victory",
   "metadata": {},
   "outputs": [],
   "source": [
    "adata.var[['GeneID', 'GeneName']].to_csv(SAVE_DIR + \"unique_genes_flt_data.csv\", index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "bioLordVenv",
   "language": "python",
   "name": "biolordvenv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}